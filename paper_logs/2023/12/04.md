## Daily Papers (2023-12-04)

### [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oerJs2Ei3GDpVry8sCCEe.png)

Authors: Albert Gu, Tri Dao

- 딥러닝 분야에서 설렘을 느끼게 하는 대부분의 응용 프로그램에 힘을 실어주는 기반 모델들은 거의 하나같이 Transformer 구조와 핵심인 주의 모듈에 기반을 두고 있습니다.
- 긴 시퀀스에서의 Transformers의 계산 비효율성을 해결하기 위해 선형 주의, 게이트 컨볼루션 및 순환 모델, 구조화된 상태 공간 모델(SSM) 등 여러 서브쿼드라틱-타임 아키텍처가 개발되었으나, 언어와 같이 중요한 모달리티에서의 성능은 주의와 같은 성과를 내지 못했습니다.
- 해당 모델들의 주요 약점 중 하나는 내용 기반 추론 수행 능력 부족이며, 이 논문은 몇 가지 개선을 제안합니다.
- 우선, SSM 파라미터를 입력의 함수로 만듦으로써 모델이 현재 토큰에 따라 시퀀스 길이 차원에 따른 정보의 전파와 잊음을 선택적으로 할 수 있도록 하여 이산 모달리티와 관련된 약점을 해결합니다.
- 둘째, 이러한 변화는 효율적인 컨볼루션 사용이 불가능하게 하지만, 우리는 재귀 모드에서 하드웨어 인식 병렬 알고리즘을 설계합니다.
- 이러한 선택적 SSM을 주의나 MLP 블록도 없이 단순화된 end-to-end 신경망 구조체(Mamba)에 통합합니다.
- Mamba는 빠른 추론(트랜스포머보다 5배 높은 처리량)과 시퀀스 길이에 대한 선형 스케일링을 자랑하며, 실제 데이터를 기반으로 백만 길이의 시퀀스까지 성능이 향상됩니다.
- 일반 시퀀스 모델 백본으로서 언어, 오디오, 유전체과학 등 다양한 모달리티에 걸쳐 최첨단 성능을 달성하는 Mamba가 제시됩니다.
- 언어 모델링에서 우리의 Mamba-3B 모델은 동일한 크기의 트랜스포머보다 성능이 우수하며, 그 크기의 두 배인 트랜스포머와 같은 성능을 보여주면서, 사전 훈련 및 다운스트림 평가 모두에서 우수한 결과를 내놓습니다.

### [Merlin:Empowering Multimodal LLMs with Foresight Minds](https://arxiv.org/abs/2312.00589)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/i0OtI7IVL_ANfE1YOKxxt.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/i0OtI7IVL_ANfE1YOKxxt.mp4" muted="false"></video></div>

Authors: En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Wenbing Tao

- 현재 관찰을 바탕으로 미래를 어느 정도 예측하는 인간의 능력인 '예견 마음'은 다모달 대형 언어 모델(MLLMs) 내에서 대부분 탐험되지 않아, 일어나는 것들과 관찰 대상의 의도 뒤에 있는 기본 원칙을 배우는 능력이 저해되었습니다.
- 이 문제를 해결하기 위해 저자들은 MLLMs의 기존 학습 프레임워크 내에 미래 모델링 통합을 소개합니다. 연속된 프레임 시퀀스의 매우 구조화된 표현인 '주제 궤적'을 학습 목표로 활용하여 과거와 미래 사이의 간극을 메우고자 합니다.
- 저자들은 두 가지 혁신적인 방법, '예견 사전-훈련(FPT)'과 '예견 지시-튜닝(FIT)'을 제안합니다. 이 방법들은 LLMs의 현대 학습 패러다임에서 영감을 받았습니다.
- 구체적으로, FPT는 궤적을 중심으로 한 다양한 과제를 공동으로 훈련함으로써 MLLMs가 주어진 초기 관찰에서 전체 궤적을 주목하고 예측하는 방법을 배울 수 있도록 합니다.
- 그 후 FIT는 MLLMs가 관련 객체의 궤적을 먼저 예측하고 그것을 바탕으로 잠재적 미래 이벤트에 대해 추론하도록 요구합니다.
- FPT와 FIT의 도움으로, 저자들은 미래 추론과 시각 이해 작업 모두에서 인상적인 성과를 보이는, 'Merlin'이라는 새로운 통합 MLLM을 구축했습니다. 이는 멀티-이미지 입력과 다수 객체의 잠재적 동작에 대한 분석을 지원합니다.

### [SeaLLMs -- Large Language Models for Southeast Asia](https://arxiv.org/abs/2312.00738)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pymnAK_Ku-76YRqJf3Dbv.png)

Authors: Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing

- 이 연구는 대규모 언어 모델(LLMs)이 고자원 언어로 편향되어 있는 문제를 해결하고자 동남아시아(SEA) 언어에 특화된 새로운 언어 모델인 SeaLLMs을 소개한다.
- SeaLLMs은 Llama-2 모델을 기반으로 하여 확장된 어휘, 전문화된 지침 및 조정 튜닝을 통해 훈련되었으며, 이는 지역 언어의 복잡성을 더 잘 포착한다.
- 이 모델들은 현지 문화 규범, 관습, 스타일 선호도 및 법적 고려사항을 존중하고 반영한다.
- SeaLLM-13b 모델은 다양한 언어적 작업과 지시사항을 따르는 능력에서 기존의 개방형 모델에 비해 우수한 성능을 보여준다.
- 특히, 태국어, 크메르어, 라오스어, 버마어와 같은 비-라틴 문자 언어에서는 ChatGPT-3.5보다 큰 격차로 성능이 우수하며, 경제적이고 효율적인 운영이 가능하다.

### [VideoBooth: Diffusion-based Video Generation with Image Prompts](https://arxiv.org/abs/2312.00777)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/E0MJ6ZziqGlAg1vx7FfXL.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/E0MJ6ZziqGlAg1vx7FfXL.mp4" muted="false"></video></div>

Authors: Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, Ziwei Liu

- 본 논문에서는 텍스트 프롬프트만을 사용하는 것으로는 사용자의 의도에 정확히 부합하는 맞춤형 콘텐츠 생성에 어려움이 있음을 인지하고, 이미지 프롬프트를 활용한 비디오 생성 작업에 대해 연구한다.
- VideoBooth라 명명된 제안된 피드포워드 프레임워크에서는 두 가지 주요 설계를 통해 이미지 프롬프트를 임베딩하는 방식을 제안한다:
- 실험을 통해 VideoBooth가 이미지 프롬프트에서 지정된 주제로 맞춤형 고품질 비디오를 생성하는데 있어 최신 성능을 달성한다는 것을 입증한다.
- 특히, VideoBooth는 다양한 이미지 프롬프트에 대해 단일 모델이 피드포워드 패스로 작동하는 일반화 가능한 프레임워크임을 강조한다.

### [Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses](https://arxiv.org/abs/2312.00763)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fnQkcnta4ez3lXLmBUeis.png)

Authors: Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, Quoc Le, Ed Chi

- 대규모 언어 모델(LLM)을 활용한 챗봇은 주로 텍스트 기반으로 사용되어, 여행 계획이나 새로운 도시에 대해 배우는 것과 같은 탐색적이거나 센스 메이킹(sensemaking) 작업에 있어 사용자에게 상당한 인지적 부담을 준다.
- 텍스트 기반 상호작용은 사용자가 구조, 정보의 "향기", 또는 고수준 선호도나 목표를 명시하는데 있어 충분한 지원을 제공하지 않는다.
- ExploreLLM은 사용자가 생각을 구조화하고 다양한 옵션을 탐색하며, 선택과 추천을 통해 탐색하고, 모델이 더 개인화된 반응을 생성하도록 쉽게 유도할 수 있게 해준다.
- 사용자 연구를 통해, ExploreLLM이 탐색적이거나 계획 작업에 사용하기에 유용하며, 작업에 스키마 같은 구조를 제공하고 계획을 세우는 데 도움을 준다는 것을 보여주었다.
- 또한, ExploreLLM을 사용하여 사용자가 고수준의 선호도를 더 쉽게 개인화된 반응으로 전환할 수 있다는 것을 사용자 연구가 제안하고 있다.
- ExploreLLM은 사용자가 LLM과 상호작용하는 미래를 가리키고 있으며, 이는 자연어와 그래픽 사용자 인터페이스 간의 긴밀한 통합을 통해 복잡한 사용자 작업을 지원하도록 설계된다.

### [GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs](https://arxiv.org/abs/2312.00093)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wmLOCLR3ddDG6ZXkiUmnW.png)

Authors: Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, Bernhard Schölkopf

- 기존의 텍스트로부터 3D 모델을 생성하는 방법들은 복잡한 장면을 묘사하는 텍스트를 효과적으로 해석하는데 한계가 있었습니다.
- GraphDreamer는 노드로 나타낸 객체와 에지로 표현된 그들의 상호작용을 포함하는 장면 그래프로부터 복합적인 3D 장면을 생성하는 새로운 프레임워크를 제안합니다.
- 이 방법은 사전 훈련된 텍스트-이미지 확산 모델을 더 효과적으로 활용하고, 이미지 수준의 감독 없이도 다양한 객체들을 완전히 분리할 수 있습니다.
- 객체 간의 상호 침투를 피하기 위해 서명된 거리 필드를 사용하고, 이를 제약 조건으로 설정합니다.
- 장면 그래프를 수동으로 만들지 않기 위해, 텍스트 입력에 기반한 장면 그래프를 생성하기 위해 ChatGPT에 대한 텍스트 프롬프트를 디자인하였습니다.
- GraphDreamer가 분리된 객체 엔티티를 가진 고품질의 복합적인 3D 장면을 생성하는 효과를 검증하기 위해 질적 및 양적 실험을 수행하였습니다.

### [MoMask: Generative Masked Modeling of 3D Human Motions](https://arxiv.org/abs/2312.00063)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ob-BfUnlpfbNY3H3N8b_f.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ob-BfUnlpfbNY3H3N8b_f.mp4" muted="false"></video></div>

Authors: Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, Li Cheng

- 'MoMask'는 텍스트 기반 3D 인간 동작 생성을 위한 새로운 마스킹 모델링 프레임워크입니다.
- 인간 동작을 다층 이산 동작 토큰으로 나타내는 계층적 양자화 방식을 사용하여 세부적인 자세들을 고해상도로 표현합니다.
- 베이스 레이어의 동작 토큰들은 벡터 양자화를 통해 얻어지며, 이후 계층에서는 점점 더 높은 순위의 잔여 토큰들이 파생됩니다.
- 텍스트 입력을 조건으로 무작위 마스킹된 동작 토큰들을 예측하는 '마스킹 트랜스포머'와 현재 계층의 결과를 바탕으로 다음 계층 토큰들을 예측하는 '잔여 트랜스포머' 두 가지 양방향 트랜스포머가 사용됩니다.
- 학습 단계에서는 텍스트 입력을 기반으로 무작위로 마스킹된 동작 토큰들을 예측하고, 생성(추론) 단계에서는 빈 시퀀스에서 시작하여 마스킹 트랜스포머가 반복적으로 누락된 토큰들을 채워 나갑니다.
- MoMask는 텍스트 대 동작 생성 과제에서 최신 방법들을 뛰어넘는 성능을 보여주며, HumanML3D 데이터셋에서 FID 0.045 (예를 들어 T2M-GPT의 0.141과 비교), KIT-ML에서는 0.228 (0.514와 비교)을 달성했습니다.
- 또한, MoMask는 추가 모델 파인튜닝 없이 관련 작업에도 원활하게 적용될 수 있으며, 텍스트 가이드 시간적 인페인팅과 같은 작업에 사용될 수 있습니다.

### [HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models](https://arxiv.org/abs/2312.00079)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ujH0DfwiNHmlle-Yscpdu.png)

Authors: Zhonghao Wang, Wei Wei, Yang Zhao, Zhisheng Xiao, Mark Hasegawa-Johnson, Humphrey Shi, Tingbo Hou

- 이 논문은 사전 훈련된 텍스트-이미지 확산 모델을 이용한 개인화된 고화질 이미지 생성의 발전을 탐구합니다.
- 기존 방법들은 텍스트 설명과 몇 장의 입력 이미지를 기반으로 다양한 장면을 생성하는 데 중요한 발전을 이루었지만, 생성된 이미지 내의 주제물의 충실도를 유지하는 데는 도전이 남아 있습니다.
- 본 연구에서는 객체의 외모 보존을 향상시키기 위해 'HiFi Tuner'라는 혁신적인 알고리즘을 제안합니다.
- 제안된 방법은 매개변수 효율적인 미세 조정 프레임워크를 사용하며, 이는 노이즈 제거 과정과 핵심 역전환 과정을 포함합니다.
- 주요 개선 사항으로는 마스크 지침 사용, 새로운 매개변수 정규화 기술, 단계적 주체 표현의 도입이 있어 샘플의 충실도를 높입니다.
- 또한, 기준 이미지의 핵심 역전을 활용하는 참조 가이드 생성 접근 방식을 제안하여 원치 않는 주제 변형과 아티팩트를 줄입니다.
- 본 연구는 텍스트 조작을 통한 이미지 내 주제물의 교체라는 새로운 이미지 편집 작업으로 방법을 확장합니다.
- 안정적인 확산 모델을 사용한 DreamBooth 데이터셋에 대한 실험 평가는 유망한 결과를 보여줍니다.
- 텍스트 임베딩만으로 미세 조정할 때 CLIP-T 점수가 3.6점, DINO 점수가 9.6점 향상되어 Textual Inversion보다 개선됩니다.
- 모든 매개변수에 대해 미세 조정할 때 HiFi Tuner는 CLIP-T 점수를 1.2점, DINO 점수를 1.2점 향상시켜 DreamBooth보다 높은 새로운 기준을 설정합니다.

### [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/c8weVBbR-oTW9F_6HKc1S.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/c8weVBbR-oTW9F_6HKc1S.mp4" muted="false"></video></div>

Authors: Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, Chaowei Xiao

- 본 논문에서는 사람과 같은 이해력과 반응성을 가진 완전 자율 주행 차량(AV)을 위한 신규 비전-언어 모델인 Dolphins를 소개합니다.
- Dolphins는 비디오(또는 이미지) 데이터, 텍스트 지시, 역사적 제어 신호 등의 다중 모드 입력을 처리하여 제공된 지시에 따른 정보를 생성하는 능력을 갖추고 있습니다.
- 오픈 소스의 사전 훈련된 비전-언어 모델인 OpenFlamingo를 기반으로 새로운 Grounded Chain of Thought (GCoT) 과정을 통해 Dolphins의 추론 능력을 향상시켰습니다.
- 또한 운전 도메인에 특화된 지시 데이터를 구성하고, 지시 튜닝을 통해서 Dolphins를 맞춤화하였습니다.
- BDD-X 데이터셋을 활용하여 Dolphins 내에 네 가지 독특한 AV 작업을 설계하고 통합함으로써 복잡한 운전 시나리오에 대한 종합적인 이해를 도모했습니다.
- Dolphins의 특징은 복잡하고 긴 꼬리를 가진 오픈 월드 운전 시나리오를 이해하고 다양한 AV 태스크를 해결하는 능력과, 맥락 학습을 통한 그라디언트-프리 즉각적인 적응력 및 반성을 통한 오류 복구 같은 인간과 유사한 능력을 나타내는 두 가지 차원으로 나뉩니다.

### [DREAM: Diffusion Rectification and Estimation-Adaptive Models](https://arxiv.org/abs/2312.00210)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/LGhmCA-2WDqeXmMR6RosX.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/LGhmCA-2WDqeXmMR6RosX.mp4" muted="false"></video></div>

Authors: Jinxin Zhou, Tianyu Ding, Tianyi Chen, Jiachen Jiang, Ilya Zharkov, Zhihui Zhu, Luming Liang

- 본 연구에서는 디퓨전 모델의 훈련과 샘플링을 정렬시키 기 위해 최소한의 코드 변경(단 세 줄)으로 상당한 개선을 이끌어낼 수 있는 새로운 훈련 프레임워크인 'DREAM'을 소개합니다.
- DREAM은 두 가지 주요 구성 요소를 포함합니다: 샘플링 과정을 반영하여 훈련을 조정하는 '디퓨전 정정(diffusion rectification)'과 인식과 왜곡 사이의 균형을 맞추는 '추정 적응(estimation adaptation)'.
- 이미지 초고해상도(SR) 작업에 적용할 때, DREAM은 왜곡을 최소화하면서도 고품질 이미지를 보존하는 사이의 균형을 능숙하게 탐색합니다.
- 실험 결과, DREAM은 표준 디퓨전 기반 SR 방법보다 월등한 성능을 입증하며, 훈련 수렴 속도가 2배에서 3배 빠르고, 비슷하거나 우수한 결과를 달성하기 위한 필요 샘플링 단계수를 10배에서 20배 감소시킵니다.
- 이 연구는 디퓨전 모델 훈련 패러다임에 대한 재고를 촉진할 것으로 기대됩니다.

### [StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter](https://arxiv.org/abs/2312.00330)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kmZhpGHTsYyUhn1gM6cI-.png)

Authors: Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Xintao Wang, Yujiu Yang, Ying Shan

- StyleCrafter는 사전 훈련된 텍스트-비디오(T2V) 모델에 스타일 제어 어댑터를 추가하여 참조 이미지를 통해 어떤 스타일에서도 비디오를 생성할 수 있는 새로운 방법을 제시합니다.
- 참조 이미지를 제공함으로써 사용자가 원하는 스타일의 비디오를 생산하기 어렵다는 문제에 주목하고, 스타일화된 비디오 데이터셋의 부족함을 고려하여 스타일 풍부한 이미지 데이터셋을 사용해 먼저 스타일 제어 어댑터를 훈련시킵니다.
- 이 학습된 스타일화 능력을 비디오 생성으로 전환하기 위해 맞춤형 파인튜닝 패러다임을 도입합니다.
- 콘텐츠와 스타일의 분리를 촉진하기 위해 텍스트 프롬프트에서 스타일 설명을 제거하고, 참조 이미지로부터만 스타일 정보를 추출하는 디커플링 학습 전략을 사용합니다.
- 또한, 텍스트 기반 콘텐츠 특성과 이미지 기반 스타일 특성의 영향을 균형 있게 혼합하는 스케일 적응형 퓨전 모듈을 설계하여 다양한 텍스트 및 스타일 조합에 대한 일반화를 돕습니다.
- StyleCrafter는 텍스트의 내용과 일치하면서 참조 이미지의 스타일을 닮은 고품질의 스타일화된 비디오를 효율적으로 생성합니다.
- 실험을 통해 우리의 접근 방식이 기존의 방식보다 더 유연하고 효율적이라는 것을 확인합니다.

### [Instruction-tuning Aligns LLMs to the Human Brain](https://arxiv.org/abs/2312.00575)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/16sriaG9lhSaqJnAgDf3l.png)

Authors: Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, Antoine Bosselut

- 인스트럭션 튜닝은 큰 규모의 언어 모델(Large Language Models, LLMs)이 자연어 질의에 대해 인간과 더 유사한 반응을 생성하도록 세밀하게 조정하는 널리 사용되는 방법으로, 다양한 실험 환경에서 인간 수준의 성능을 이끌어냈습니다.
- 이 연구는 인스트럭션 튜닝이 인간이 언어를 처리하는 방식과 LLMs의 유사성을 증진시키는지 여부를 두 가지 방법으로 조사하였습니다: (1) 뇌 정렬(인간의 언어 체계에서 발생하는 신경 활동과 LLM 내부 표현의 유사성) 및 (2) 행동 정렬(읽기 작업에서 LLM과 인간 행동의 유사성).
- 자연적인 이야기와 문장을 읽는 인간을 대상으로 한 세 가지 데이터셋을 통해 25개의 일반적인 및 인스트럭션 튜닝된 LLMs를 평가한 결과, 인스트럭션 튜닝이 일반적으로 뇌 정렬을 평균 6% 향상시키지만, 행동 정렬에는 비슷한 영향을 주지 않았음을 발견하였습니다.
- LLMs의 뇌 정렬과 연결된 다양한 모델 속성들 사이의 상관관계를 분석함으로써, LLM-뇌 정렬에 기여하는 요인을 파악하였고, 모델 크기와 세계 지식을 요구하는 작업에서의 성능이 각각 (r = 0.95), (r = 0.81)로 높은 양의 상관관계가 있음을 발견하였습니다.
- 이 결과는 인스트럭션 튜닝이 세계 지식 표현과 인간 뇌와의 정렬을 개선하며, 세계 지식을 인코딩하는 메커니즘이 LLMs의 인간 뇌와의 대표성 정렬을 향상시키는 것을 시사합니다.

### [PyNeRF: Pyramidal Neural Radiance Fields](https://arxiv.org/abs/2312.00252)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/z8L8wOJOlfurOP-Wwpcm5.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/z8L8wOJOlfurOP-Wwpcm5.mp4" muted="false"></video></div>

Authors: Haithem Turki, Michael Zollhöfer, Christian Richardt, Deva Ramanan

- 신경 복사 필드(NeRF)는 공간 그리드 표현을 통해 속도를 크게 향상시킬 수 있지만, 카메라 거리에 다르게 캡처된 장면을 재구성할 때 직접적인 스케일 고려가 없어 에일리어싱 현상을 발생시킨다.
- Mip-NeRF 및 확장 버전은 점 샘플이 아닌 볼륨 프러스텀을 투영하는 스케일 인식 렌더러를 제안하지만, 이러한 접근법은 그리드 방식과 잘 호환되지 않는 위치 인코딩에 의존한다.
- 다른 공간 그리드 해상도에서 모델 헤드를 훈련함으로써 그리드 기반 모델에 간단한 변경을 제안한다.
- 렌더링 시에는 더 넓은 볼륨을 커버하는 샘플을 렌더링하기 위해 더 거친 그리드를 사용한다.
- 본 방법은 기존 가속화된 NeRF 방법에 쉽게 적용될 수 있으며, 실제로 미미한 성능 오버헤드로 렌더링 품질(합성 및 실제 무한한 장면에 걸쳐 오류율을 20-90% 감소)을 크게 향상시킨다.
- Mip-NeRF와 비교하여 오류율을 20% 감소시키면서 60배 더 빠르게 훈련한다.

### [Towards Accurate Differential Diagnosis with Large Language Models](https://arxiv.org/abs/2312.00164)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AZoDiVVCnZ4rPJlwE_a-B.png)

Authors: Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S Sara Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak Patel, Dale R Webster, Ewa Dominowska, Juraj Gottweis, Joelle Barral, +

- 정확한 감별진단(DDx)은 의학적 관리의 핵심이며, 임상 병력, 신체 검사, 조사 및 절차의 해석을 결합하는 순환 과정을 통해 도달됩니다.
- 대규모 언어 모델(LLMs)을 활용한 인터랙티브 인터페이스는 이 과정을 돕거나 자동화하는 새로운 기회를 제공합니다.
- 본 연구에서는 진단 추론에 최적화된 LLM을 소개하고, 혼자서 또는 의료진의 보조로 감별진단을 생성하는 능력을 평가하였습니다.
- 20명의 의료진이 뉴잉글랜드 의학 저널(NEJM) 사례 보고서에서 가져온 302건의 어려운 실제 의료 사례를 평가하였습니다.
- 이들은 검색 엔진과 표준 의료 자원을 이용하거나 이러한 도구뿐만 아니라 LLM의 보조를 받는 두 가지 상태로 무작위화되어 각 사례 보고서를 읽었습니다.
- 모든 의료진은 보조 도구를 사용하기 전에 기본적인, 무보조 감별진단을 제공했으며, LLM에 의한 DDx는 무보조 의료진보다 뛰어난 성능을 보였습니다(상위 10개 정확도 59.1% 대 33.6%, [p = 0.04]).
- 두 가지 보조 조건을 비교했을 때, LLM의 도움을 받은 의료진이 그렇지 않은 의료진(상위 10개 정확도 36.1%) (McNemar's Test: 45.7, p < 0.01) 또는 검색 도움을 받은 의료진(44.4%) (4.75, p = 0.03)보다 더 높은 DDx 품질 점수를 받았습니다.
- LLM의 도움을 받은 의료진은 그렇지 않은 의료진보다 더 포괄적인 감별 리스트를 작성하였습니다.
- 본 연구는 복잡한 사례에서 의료진의 진단 추론과 정확도를 향상시킬 수 있는 LLM에 대한 잠재력을 제시하며, 의사들을 강화시키고 환자들에게 전문가 수준의 전문 지식에 대한 접근을 확대할 수 있는 능력에 대한 추가적인 현실 세계 평가를 요구합니다.

### [FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting](https://arxiv.org/abs/2312.00451)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/leuKyWwKrMqP9W6ZaWVJl.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/leuKyWwKrMqP9W6ZaWVJl.mp4" muted="false"></video></div>

Authors: Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang

- 적은 관측치로부터 실시간이면서 사실적인 시점 합성을 가능하게 하는 FSGS(Few-shot View Synthesis using Gaussian Splatting) 프레임워크를 제안합니다.
- 제안된 FSGS 방법은 매우 희소한 초기 SfM(Structure from Motion) 점들을 다루기 위해 신중하게 설계된 가우시안(Gaussian) 언풀링 프로세스를 사용합니다.
- 최적의 솔루션을 향해 기하학적 최적화를 안내하기 위해 대규모 사전 훈련된 단안식 깊이 추정기(monocular depth estimator)를 가우시안 최적화 과정에 통합합니다.
- 제한된 입력 시점에서 관찰된 희소한 점들로부터 출발하여, FSGS는 볼 수 없는 영역까지 정확하게 확장하며 장면을 포괄적으로 커버하고 새로운 시점의 렌더링 품질을 향상시킵니다.
- FSGS는 LLFF, Mip-NeRF360, Blender 등 다양한 데이터셋에서 정확도와 렌더링 효율성 측면에서 최신(state-of-the-art) 성능을 달성합니다.
- 프로젝트 웹사이트를 통해 추가 정보와 자료를 확인할 수 있습니다: https://zehaozhu.github.io/FSGS/.

### [Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering](https://arxiv.org/abs/2312.00109)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bkqZ865RNIcFK30Hzr4E1.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bkqZ865RNIcFK30Hzr4E1.mp4" muted="false"></video></div>

Authors: Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai

- 신경망 렌더링 방법이 여러 학계 및 산업 애플리케이션에서 사실적인 3D 장면 렌더링을 크게 발전시켰다.
- 최근의 3D 가우스 스플래팅 방법은 원시 기반 표현과 볼륨 기반 표현의 장점을 결합하여 최고의 렌더링 품질과 속도를 달성했다.
- 그러나 이 방법은 종종 장면 기하학을 무시하고 모든 학습 뷰를 맞추기 위해 과도하게 중복된 가우스를 사용하여 뷰 변경, 질감이 없는 영역 및 조명 효과에 대해 덜 견고해진다.
- Scaffold-GS는 지역 3D 가우스를 분배하기 위해 앵커 지점을 사용하고, 뷰 프러스텀 내의 시청 방향 및 거리에 기초하여 그들의 속성을 즉석에서 예측한다.
- 장면 커버리지를 신뢰할 수 있게 향상시키기 위해 신경 가우스의 중요성을 근거로 앵커 성장과 정리 전략을 개발했다.
- 이 방법은 불필요한 가우스를 효과적으로 줄이면서 고품질의 렌더링을 제공한다는 점을 보여준다.
- 다양한 디테일 수준과 뷰 종속적 관찰이 있는 장면을 수용하는 향상된 능력을 보여주며, 렌더링 속도를 희생하지 않는다.

### [Text-Guided 3D Face Synthesis -- From Generation to Editing](https://arxiv.org/abs/2312.00375)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yqDGwmUl05vL6ZF0sZNk2.png)

Authors: Yunjie Wu, Yapeng Meng, Zhipeng Hu, Lincheng Li, Haoqian Wu, Kun Zhou, Weiwei Xu, Xin Yu

- 텍스트 기반 3D 얼굴 합성은 이미지 생성에 초점을 맞추지만 편집은 고려하지 않아, 반복적인 조정을 통한 맞춤형 3D 얼굴을 만드는데 제한적이었다.
- 본 논문에서는 얼굴 생성부터 편집까지 아우르는 통합적인 텍스트 유도 프레임워크를 제안한다.
- 생성 단계에서는 기하학과 텍스처를 분리함으로써 결합으로 인한 기하학적 상세 손실을 완화하고, 생성된 기하학을 텍스처 생성의 조건으로 사용해 높은 정렬 결과를 얻을 수 있다.
- 또한, RGB와 YUV 공간 모두에서 텍스처 품질을 향상하기 위하여 미세 조정된 텍스처 확산 모델을 사용한다.
- 편집 단계에서는 미리 훈련된 확산 모델을 사용하여 텍스트에 기반한 얼굴 기하학이나 텍스처를 업데이트한다.
- 순차적 편집을 가능하게 하기 위해 UV 도메인 일관성 보존 규제를 도입하여 무관한 얼굴 속성에 대한 변화를 방지한다.
- 일관성을 유지하면서 편집 효과를 향상시키기 위한 자체 안내 일관성 가중치 전략을 제안한다.
- 종합적인 실험을 통해 본 방법의 얼굴 합성에 있어 우수성을 입증한다.
- 프로젝트 페이지 주소: https://faceg2e.github.io/.

### [X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation](https://arxiv.org/abs/2312.00085)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qE12aHycawbiPkQdNhLmb.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qE12aHycawbiPkQdNhLmb.mp4" muted="false"></video></div>

Authors: Yiwei Ma, Yijun Fan, Jiayi Ji, Haowei Wang, Xiaoshuai Sun, Guannan Jiang, Annan Shu, Rongrong Ji

- 최근 자동 텍스트-투-3D 콘텐츠 생성이 사전 학습된 2D 확산 모델의 발달에 힘입어 상당한 진보를 이룩했다.
- 기존의 텍스트-투-3D 방법은 3D 표현을 최적화하여 사전에 학습된 2D 확산 모델에 의해 평가된 텍스트와 잘 일치하는 이미지를 렌더링하는 것을 목표로 한다.
- 하지만, 카메라 관련 속성과 전경 객체의 독점적 존재로 인해 2D 이미지와 3D 자산 사이에는 상당한 도메인 차이가 존재한다.
- 이를 해결하기 위해 고품질 텍스트-투-3D 콘텐츠 생성을 위해 도메인 간 격차를 연결하는 X-Dreamer라는 새로운 접근법을 제시한다.
- X-Dreamer의 핵심 구성요소는 Camera-Guided Low-Rank Adaptation (CG-LoRA)과 Attention-Mask Alignment (AMA) 손실이라는 두 가지 혁신적인 설계이다.
- CG-LoRA는 카메라에 의존하는 생성 파라미터에 대해 훈련이 가능하며, 카메라 정보를 사전 학습된 확산 모델에 동적으로 통합하여 생성된 3D 자산과 카메라 관점 간의 일치를 향상시킨다.
- AMA 손실은 사전 학습된 확산 모델의 주의 맵을 3D 객체의 이진 마스크를 사용하여 안내함으로써, 전경 객체의 생성에 집중하도록 한다.
- 이 모듈은 모델이 정확하고 상세한 전경 객체를 생성하는 데 집중할 수 있도록 보장한다.
- 방대한 평가를 통해 우리의 제안한 방법이 기존 텍스트-투-3D 접근법에 비해 효과적임을 입증한다.
- 프로젝트 웹페이지 주소는 다음과 같다: https://xmuxiaoma666.github.io/Projects/X-Dreamer .
