## Daily Papers (2023-11-23)

### [GAIA: a benchmark for General AI Assistants](https://arxiv.org/abs/2311.12983)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0AEaFLBo3vO3BM3CsYsTC.png)

Authors: Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, Thomas Scialom

- GAIA 벤치마크는 추론, 다중 모드 처리, 웹 검색 및 도구 사용 능력 같은 기본적인 능력을 필요로 하는 현실 세계 질문들을 제안하여 AI 연구에서 중요한 이정표가 될 것을 목표로 합니다.
- 이 벤치마크에 포함된 질문들은 인간에게는 개념적으로 간단하지만 플러그인이 탑재된 GPT-4와 같은 최신 AI 시스템에게는 도전적입니다; 인간 응답자는 평균 92%의 점수를 획득한 반면 GPT-4는 15%에 불과했습니다.
- 이같은 성능 격차는 법학이나 화학 등 전문적인 기술을 요구하는 태스크에서 인간을 능가하는 최신 대규모 언어 모델(Large Language Models, LLMs)의 최근 경향과 대조를 이룹니다.
- GAIA의 철학은 인간에게 점점 더 어려운 태스크를 목표로 하는 현재 AI 벤치마크의 추세에서 벗어나, 인공 일반 지능(Artificial General Intelligence, AGI)의 출현은 평균적인 인간이 이러한 질문에 보이는 유사한 강인함을 시스템이 나타내야 함을 전제로 합니다.
- GAIA의 방법론을 사용하여, 연구자들은 466개의 질문과 그에 대한 답을 고안했으며, 이 중 300개의 답을 보유하고 있어 https://huggingface.co/gaia-benchmark 에서 사용할 수 있는 리더보드를 제공합니다.

### [Diffusion Model Alignment Using Direct Preference Optimization](https://arxiv.org/abs/2311.12908)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pi324wWiMEwY3iCA7LrUd.png)

Authors: Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik

- 대규모 언어 모델(LLMs)을 사용자 선호도와 더 잘 맞도록 하기 위해 인간 비교 데이터를 이용한 강화 학습(RLHF) 방법으로 미세 조정이 이루어졌습니다.
- 그와 대조적으로, 텍스트-이미지 확산 모델에서 인간의 선호 학습은 광범위하게 탐구되지 않았고, 기존의 가장 좋은 방법은 고품질 이미지와 캡션을 이용한 사전 훈련된 모델의 미세 조정입니다.
- 연구팀은 인간 비교 데이터에 직접 최적화를 수행하여 확산 모델을 인간 선호도에 맞추는 Diffusion-DPO 방법을 제안합니다.
- 본 논문에서는 최근에 개발된 직접 선호 최적화(DPO)를 적용하여, 분류 목적 하에서 인간 선호도를 가장 잘 만족시키는 정책을 직접 최적화합니다.
- 연구팀은 확산 모델 개념의 가능성을 고려하여 DPO를 재구성하고, 증거 하한을 활용하여 미분 가능한 목표를 도출합니다.
- Pick-a-Pic 데이터셋의 851K 쌍의 선택 기반 선호도를 사용하여, 최첨단 Stable Diffusion XL (SDXL)-1.0 모델의 기본 모델을 Diffusion-DPO로 미세 조정합니다.
- 미세 조정된 기본 모델은 인간 평가에서 SDXL-1.0 기본 모델과 추가 정제 모델을 포함한 더 큰 SDXL-1.0 모델 모두를 뛰어넘어 시각적 매력과 프롬프트 정렬을 개선합니다.
- 또한 인공지능 피드백을 사용하는 변형을 개발하여, 인간 선호도에 대한 교육 성능과 비교할만한 성능을 보여주며 확산 모델 정렬 방법의 규모 확장 가능성을 제시합니다.

### [LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes](https://arxiv.org/abs/2311.13384)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/cqiIO-xwwkD699LC0bfZc.png)

Authors: Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee

- VR 기기 및 콘텐츠의 보급과 함께, 3D 장면 생성 기술에 대한 수요가 증가하고 있음에도 불구하고, 현재의 모델들은 주로 실세계와 거리가 먼 3D 스캔 데이터셋을 사용하여 특정 도메인에 한정된 장면을 생성하는 데에 제한이 있습니다.
- 이러한 한계를 해결하기 위해, 기존 대규모 확산 기반 생성 모델의 잠재력을 최대한 활용하는 도메인-프리(Domain-free) 장면 생성 파이프라인인 LucidDreamer를 제안합니다.
- LucidDreamer는 'Dreaming'과 'Alignment'라는 두 단계를 번갈아가며 수행합니다. 먼저, 입력으로부터 다중 시점의 일관된 이미지를 생성하기 위해 점 구름을 각 이미지 생성에 대한 기하학적 가이드라인으로 설정합니다.
- 특정 시점으로 점 구름의 일부를 투영하여 생성 모델을 이용한 인페인팅에 대한 가이드로 제공하며, 인페인팅된 이미지는 추정된 깊이 맵과 함께 3D 공간으로 상승되어 새로운 점들을 구성합니다.
- 새로이 생성된 3D 장면의 조각들을 3D 장면에 효과적으로 통합하기 위해 조화롭게 접목하는 정렬 알고리즘을 제안합니다.
- 최종적으로 얻은 3D 장면은 가우시안 스플렛(Gaussian splats)을 최적화하기 위한 초기 점으로서 사용되며, LucidDreamer는 이전 3D 장면 생성 방법들과 비교해 높은 디테일을 가진 가우시안 스플렛을 생성하되, 생성하려는 장면의 도메인에 대한 제약 없이 수행합니다.

### [Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model](https://arxiv.org/abs/2311.13231)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6zV91fOTg9PR19IYMviHg.png)

Authors: Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, Xiu Li

- 인간의 피드백을 사용한 강화 학습(RLHF)은 확산 모델을 미세 조정하는 데 큰 가능성을 보여왔으나, 효율적인 보상 모델을 만드는 과정은 방대한 데이터셋, 최적 구조, 수동 하이퍼파라미터 튜닝이 필요하여 시간과 비용이 많이 든다.
- 직접적 선호 최적화(DPO) 방법은 보상 모델이 필요 없이 대규모 언어 모델을 미세 조정하는 데 효과적이지만, 확산 모델의 잡음 제거 과정에서 요구되는 방대한 GPU 메모리 때문에 직접 적용이 어렵다.
- 본 논문은 보상 모델을 직접 훈련하지 않고도 확산 모델을 곧바로 미세 조정할 수 있는 Direct Preference for Denoising Diffusion Policy Optimization (D3PO) 방법을 소개한다.
- 이론적 분석을 통해 D3PO가 보상 모델을 훈련하지 않음에도 불구하고, 마치 인간 피드백 데이터를 사용하여 훈련된 최적의 보상 모델로 동작하여 학습 과정을 안내한다고 증명되었다.
- D3PO 방법은 보상 모델 훈련이 불필요하며, 더 직접적이고 비용 효율적이며 계산 리소스를 줄이는 장점이 있다.
- 실험에서는 인간 선호도의 대리 지표로 목표치의 상대적 스케일을 사용하여, 실제 보상을 사용하는 메소드와 비교할 만한 결과를 도출했다.
- 또한, D3PO는 견고한 보상 모델이 없는 도전을 극복하고 이미지 왜곡율을 낮추며, 더 안전한 이미지 생성능력을 보여주었다.

### [FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline](https://arxiv.org/abs/2311.13073)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PdO3FTxhxP68aqevsJIaB.gif)

Authors: Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov

- 이 연구에서는 텍스트에서 이미지로 생성하는 모델을 바탕으로 한 새로운 두 단계 잠재 확산 텍스트에서 비디오로 생성하는 아키텍처를 제시합니다.
- 첫 번째 단계는 비디오의 스토리라인을 구상하는 핵심 프레임의 생성에 관한 것이며, 두 번째 단계는 장면과 객체의 움직임을 부드럽게 만들기 위한 보간 프레임 생성에 전념합니다.
- 저자들은 핵심 프레임 생성을 위한 여러 시간 조건화 접근법을 비교하여, 시간 블록을 사용하는 것이 비디오 생성 품질 측면과 인간의 선호도를 반영하는 지표에서 시간 층을 사용하는 것보다 유리함을 보여줍니다.
- 보간 모델의 설계는 다른 마스킹된 프레임 보간 접근법에 비해 계산 비용을 상당히 줄입니다.
- 또한, MoVQ 기반 비디오 디코딩 구성의 다양한 설정을 평가함으로써 일관성을 향상시키고 더 높은 PSNR, SSIM, MSE, 및 LPIPS 점수를 달성하도록 합니다. 
- 마지막으로, 저자들은 기존 솔루션과 비교하여 전체적으로 상위 2점을, 그리고 오픈소스 솔루션 중에서는 최고점인 CLIPSIM = 0.2976 그리고 FVD = 433.054의 점수를 얻음으로써, 그들의 파이프라인의 우수성을 입증합니다.
- 프로젝트 페이지 : https://ai-forever.github.io/kandinsky-video/

### [ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs](https://arxiv.org/abs/2311.13600)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hQ5rjnbiKJMBrme8qEWTv.png)

Authors: Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, Varun Jampani

- 최근에 제안된 low-rank adaptations(LoRA)는 개념 기반 개인화를 위한 효율적인 방법으로서 기존의 방법들이 주제나 스타일 생성에서만 강력한 결과를 보였던 것에 비해, 이를 개선하고자 한다.
- 별도로 훈련된 스타일 및 주제 LoRA들을 효과적으로 결합하여 사용자가 제공한 어떤 주제나 스타일에서도 생성 가능한 ZipLoRA 방법을 제안한다.
- 실험 결과는 ZipLoRA가 다양한 주제 및 스타일 조합에서 주제 및 스타일의 충실도에 있어 기준 모델들 대비 의미 있는 개선을 이루었으며 재맥락화 능력도 유지하는 등, 인상적인 결과를 생성할 수 있음을 보여준다.
- 프로젝트 페이지(https://ziplora.github.io)에서 ZipLoRA에 대한 추가 정보와 자료를 확인할 수 있다.

### [Diffusion360: Seamless 360 Degree Panoramic Image Generation based on Diffusion Models](https://arxiv.org/abs/2311.13141)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/U80tChnjtMwgVOt_K6YRs.png)

Authors: Mengyang Feng, Jinlin Liu, Miaomiao Cui, Xuansong Xie

- 본 기술 보고서는 이산화 모델을 기반으로 한 360도 파노라마 이미지 생성 작업에 관한 것이다.
- 360도 파노라마 이미지는 전체 360도 x 180도 시야를 캡처하기 때문에, 이러한 이미지의 가장 오른쪽과 가장 왼쪽 측면이 연속되어야 한다는 것이 주된 도전 과제이다.
- 현재 이산화 파이프라인은 이러한 매끄러운 360도 파노라마 이미지를 생성하는 데 적합하지 않다.
- 이 문제를 해결하기 위해, 저자들은 기하학적 연속성을 유지하기 위해 소음 제거와 VAE 디코딩 단계에서 원형 블렌딩 전략을 제안한다.
- 이를 바탕으로, '텍스트-투-360-파노라마(Text-to-360-panoramas)'와 '싱글-이미지-투-360-파노라마(Single-Image-to-360-panoramas)' 작업을 위한 두 가지 모델을 소개한다.
- 관련 코드는 오픈 소스 프로젝트로 https://github.com/ArcherFMY/SD-T2I-360PanoImage 와 https://www.modelscope.cn/models/damo/cv_diffusion_text-to-360panorama-image_generation/summary{ModelScope}에서 공개되었다.

### [PG-Video-LLaVA: Pixel Grounding Large Video-Language Models](https://arxiv.org/abs/2311.13435)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SFgb8RMduajAvRjWnkQ-d.png)

Authors: Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, Fahad Khan

- 동영상 데이터의 복잡성으로 인해 이미지 기반의 큰 다중 모달 모델을 동영상으로 확장하는 것은 어려운 과제입니다.
- 일부 최근 접근방식들은 이미지 기반 LMM을 비디오로 확장할 때 객체 지상화 기능을 갖추지 못했거나 (예: VideoChat, Video-ChatGPT, Video-LLaMA) 동영상 이해에 도움이 되는 오디오 신호를 활용하지 않았습니다 (예: Video-ChatGPT).
- 이러한 격차를 해소하기 위해, 저희는 픽셀 수준 지상화 기능을 가진 최초의 LMM, Video-LLaVA를 제안합니다. 이 모델은 오디오 신호를 텍스트로 변환하여 비디오 맥락 이해를 풍부하게 합니다.
- Video-LLaVA는 맞춤형 추적기와 새로운 지상화 모듈을 사용하여, 사용자의 지시에 따라 비디오 내의 객체들을 공간적 및 시간적으로 정확히 찾아냅니다.
- 우리는 Video-LLaVA를 비디오 기반 생성 및 질문-응답 벤치마크에서 평가하고, 비디오 내에서 프롬프트 기반 객체 지상화 성능을 측정하기 위해 특별히 설계된 새로운 벤치마크를 소개합니다.
- 비디오 기반 대화 벤치마킹을 위해 Video-ChatGPT에서 사용된 GPT-3.5 대신 Vicuna를 사용할 것을 제안하며, 이는 GPT-3.5의 독점적인 성격으로 인한 결과 재현성 문제를 보장합니다.
- 우리의 프레임워크는 최신의 이미지 기반 LLaVA 모델에 기반을 두고 있으며, 이를 비디오 도메인으로 확장하여 비디오 기반 대화 및 지상화 작업에서 유망한 성능 향상을 제공합니다.
- 프로젝트 페이지: https://github.com/mbzuai-oryx/Video-LLaVA

### [Visual In-Context Prompting](https://arxiv.org/abs/2311.13601)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JATQYyrVl6Uy2XlOZmjOx.png)

Authors: Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao

- 대형 언어 모델(Large Language Models, LLMs)에서 널리 사용되는 In-context 프롬프팅 기법이 시각 도메인에서는 덜 탐구되었는데, 기존의 시각 프롬프팅 방법들은 가장 관련 있는 객체를 세분화하기 위해 참조 세분화(referring segmentation)에 집중하고 있습니다.
- 이 논문에서는 오픈 세트 세분화와 탐지와 같은 다양한 일반적인 시각 작업에 대한 해결 방법을 제공하는 범용 시각적 인-컨텍스트 프롬프팅 프레임워크를 소개합니다.
- 특히, 인코더-디코더 아키텍처를 기반으로 다양한 프롬프트(예: 스트로크, 박스, 포인트)를 지원하는 범용 프롬프트 인코더를 개발했으며, 임의의 수의 참조 이미지 세그먼트를 컨텍스트로 수용하는 기능을 추가했습니다.
- 이 논문에서 제안한 시각적 인-컨텍스트 프롬프팅은 획기적인 참조 및 일반 세분화 능력을 발휘하여 경쟁력 있는 성능을 닫힌 세트 인-도메인 데이터셋들에 대해 보여주었고, 많은 오픈 세트 세분화 데이터셋에서도 유망한 결과를 보여주었습니다.
- COCO와 SA-1B 데이터셋에서 공동 훈련을 받은 모델은 COCO에서 57.7 PQ, ADE20K에서 23.2 PQ의 성능을 달성했습니다.
- 관련 코드는 https://github.com/UX-Decoder/DINOv 에서 공개할 예정입니다.

